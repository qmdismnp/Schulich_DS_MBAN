{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/364\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8591 - loss: 0.5951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:37:48.192443: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 5 which is outside the valid range of [0, 5).  Label values: 0 0 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/hj/50ynckj97xq7d49_2_s66f100000gn/T/ipykernel_55135/1865235275.py\", line 53, in <module>\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in one_step_on_data\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 59, in train_step\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 399, in _compute_loss\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 367, in compute_loss\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 692, in __call__\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 701, in call\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 67, in __call__\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 33, in call\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 2241, in sparse_categorical_crossentropy\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1841, in sparse_categorical_crossentropy\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 714, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 5).  Label values: 0 0 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_multi_step_on_iterator_351963]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m rnn_optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     52\u001b[0m rnn_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mrnn_optimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 53\u001b[0m \u001b[43mrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m rnn_loss, rnn_accuracy \u001b[38;5;241m=\u001b[39m rnn_model\u001b[38;5;241m.\u001b[39mevaluate(X_test_pad, y_test)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN Model - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrnn_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrnn_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/hj/50ynckj97xq7d49_2_s66f100000gn/T/ipykernel_55135/1865235275.py\", line 53, in <module>\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in one_step_on_data\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 59, in train_step\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 399, in _compute_loss\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 367, in compute_loss\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 692, in __call__\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 701, in call\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 67, in __call__\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 33, in call\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 2241, in sparse_categorical_crossentropy\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1841, in sparse_categorical_crossentropy\n\n  File \"/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 714, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 5).  Label values: 0 0 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_multi_step_on_iterator_351963]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/static/public/911/data.csv')\n",
    "\n",
    "\n",
    "# Inspect and preprocess the dataset\n",
    "# Normalize 'best_score' to classify into 5 classes: \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "score_bins = [0, 200, 400, 600, 800, 964]\n",
    "score_labels = [0, 1, 2, 3, 4]  # Use numerical labels to avoid encoding issues\n",
    "data['best_score_class'] = pd.cut(data['best_score'], bins=score_bins, labels=score_labels)\n",
    "\n",
    "# Combine review text and target variable\n",
    "texts = data['text'].astype(str)\n",
    "labels = data['best_score_class']\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad the text\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Embedding dimensions\n",
    "embedding_dim = 128\n",
    "\n",
    "# RNN Model\n",
    "print(\"\\nTraining RNN model...\")\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "rnn_model.add(SimpleRNN(128))\n",
    "rnn_model.add(Dense(5, activation='softmax'))\n",
    "rnn_optimizer = Adam(learning_rate=0.001)\n",
    "rnn_model.compile(optimizer=rnn_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test)\n",
    "print(f\"RNN Model - Loss: {rnn_loss}, Accuracy: {rnn_accuracy}\")\n",
    "\n",
    "# BiRNN Model\n",
    "print(\"\\nTraining BiRNN model...\")\n",
    "birnn_model = Sequential()\n",
    "birnn_model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "birnn_model.add(Bidirectional(SimpleRNN(128)))\n",
    "birnn_model.add(Dense(5, activation='softmax'))\n",
    "birnn_optimizer = Adam(learning_rate=0.001)\n",
    "birnn_model.compile(optimizer=birnn_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "birnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "birnn_loss, birnn_accuracy = birnn_model.evaluate(X_test_pad, y_test)\n",
    "print(f\"BiRNN Model - Loss: {birnn_loss}, Accuracy: {birnn_accuracy}\")\n",
    "\n",
    "# LSTM Model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(5, activation='softmax'))\n",
    "lstm_optimizer = Adam(learning_rate=0.001)\n",
    "lstm_model.compile(optimizer=lstm_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_pad, y_test)\n",
    "print(f\"LSTM Model - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}\")\n",
    "\n",
    "# Save the label encoder for inference\n",
    "import pickle\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save the tokenizer for inference\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.8688 - loss: 0.5572 - val_accuracy: 0.8812 - val_loss: 0.4997\n",
      "Epoch 2/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.8738 - loss: 0.5034 - val_accuracy: 0.8804 - val_loss: 0.5363\n",
      "Epoch 3/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.8885 - loss: 0.3803 - val_accuracy: 0.8697 - val_loss: 0.5954\n",
      "Epoch 4/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9258 - loss: 0.2474 - val_accuracy: 0.8389 - val_loss: 0.7209\n",
      "Epoch 5/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9526 - loss: 0.1652 - val_accuracy: 0.8062 - val_loss: 0.8288\n",
      "\n",
      "Training BiRNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.8640 - loss: 0.5612 - val_accuracy: 0.8812 - val_loss: 0.5040\n",
      "Epoch 2/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 33ms/step - accuracy: 0.8775 - loss: 0.4711 - val_accuracy: 0.8809 - val_loss: 0.5427\n",
      "Epoch 3/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 33ms/step - accuracy: 0.8817 - loss: 0.4174 - val_accuracy: 0.8796 - val_loss: 0.5167\n",
      "Epoch 4/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - accuracy: 0.8720 - loss: 0.4883 - val_accuracy: 0.8787 - val_loss: 0.5436\n",
      "Epoch 5/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - accuracy: 0.8761 - loss: 0.4774 - val_accuracy: 0.8639 - val_loss: 0.5717\n",
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.8664 - loss: 0.6108 - val_accuracy: 0.8812 - val_loss: 0.4879\n",
      "Epoch 2/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.8791 - loss: 0.4770 - val_accuracy: 0.8812 - val_loss: 0.4940\n",
      "Epoch 3/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.8759 - loss: 0.4440 - val_accuracy: 0.8793 - val_loss: 0.5204\n",
      "Epoch 4/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9008 - loss: 0.3492 - val_accuracy: 0.8633 - val_loss: 0.5536\n",
      "Epoch 5/5\n",
      "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.9153 - loss: 0.3016 - val_accuracy: 0.8600 - val_loss: 0.6013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/static/public/911/data.csv')\n",
    "\n",
    "# Inspect and preprocess the dataset\n",
    "# Normalize 'best_score' to classify into 5 classes: \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "score_bins = [0, 200, 400, 600, 800, 964]\n",
    "score_labels = [0, 1, 2, 3, 4]  # Use numerical labels to avoid encoding issues\n",
    "data['best_score_class'] = pd.cut(data['best_score'], bins=score_bins, labels=score_labels, include_lowest=True)\n",
    "\n",
    "# Combine review text and target variable\n",
    "texts = data['text'].astype(str)\n",
    "labels = data['best_score_class'].astype(int)  # Ensure labels are integers\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad the text\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Embedding dimensions\n",
    "embedding_dim = 128\n",
    "\n",
    "# RNN Model\n",
    "print(\"\\nTraining RNN model...\")\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    SimpleRNN(128, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_history = rnn_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# BiRNN Model\n",
    "print(\"\\nTraining BiRNN model...\")\n",
    "birnn_model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(SimpleRNN(128, activation='tanh')),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "birnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "birnn_history = birnn_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# LSTM Model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(128, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_history = lstm_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validating RNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "RNN Cross-Validation F1 Score: 0.20779403586532816 (+/- 0.006787435585328952)\n",
      "\n",
      "Cross-validating BiRNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "BiRNN Cross-Validation F1 Score: 0.20759809846920874 (+/- 0.009690370643322023)\n",
      "\n",
      "Cross-validating LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "LSTM Cross-Validation F1 Score: 0.20410120864244669 (+/- 0.0038513337513023517)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Function to create RNN model\n",
    "def create_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        SimpleRNN(128, activation='tanh'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to create BiRNN model\n",
    "def create_birnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        Bidirectional(SimpleRNN(128, activation='tanh')),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        LSTM(128, activation='tanh'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Cross-validation setup\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function for cross-validation with F1 score\n",
    "def cross_validate_model(create_model_func, X, y):\n",
    "    X = np.array(X)  # Ensure X is a NumPy array\n",
    "    y = np.array(y)  # Ensure y is a NumPy array\n",
    "    f1_scores = []\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        model = create_model_func()\n",
    "        model.fit(X[train_idx], y[train_idx], epochs=5, batch_size=32, verbose=0)\n",
    "        y_pred = np.argmax(model.predict(X[test_idx]), axis=1)\n",
    "        f1 = f1_score(y[test_idx], y_pred, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "    return np.mean(f1_scores), np.std(f1_scores)\n",
    "\n",
    "# Cross-validate RNN\n",
    "print(\"\\nCross-validating RNN model...\")\n",
    "rnn_f1_mean, rnn_f1_std = cross_validate_model(create_rnn_model, X_train_pad, y_train)\n",
    "print(f\"RNN Cross-Validation F1 Score: {rnn_f1_mean} (+/- {rnn_f1_std})\")\n",
    "\n",
    "# Cross-validate BiRNN\n",
    "print(\"\\nCross-validating BiRNN model...\")\n",
    "birnn_f1_mean, birnn_f1_std = cross_validate_model(create_birnn_model, X_train_pad, y_train)\n",
    "print(f\"BiRNN Cross-Validation F1 Score: {birnn_f1_mean} (+/- {birnn_f1_std})\")\n",
    "\n",
    "# Cross-validate LSTM\n",
    "print(\"\\nCross-validating LSTM model...\")\n",
    "lstm_f1_mean, lstm_f1_std = cross_validate_model(create_lstm_model, X_train_pad, y_train)\n",
    "print(f\"LSTM Cross-Validation F1 Score: {lstm_f1_mean} (+/- {lstm_f1_std})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      3205\n",
      "           1       0.08      0.05      0.06       254\n",
      "           2       0.04      0.04      0.04        52\n",
      "           3       0.01      0.01      0.01        86\n",
      "           4       0.04      0.03      0.03        40\n",
      "\n",
      "    accuracy                           0.81      3637\n",
      "   macro avg       0.21      0.21      0.21      3637\n",
      "weighted avg       0.78      0.81      0.79      3637\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2915  148   49   72   21]\n",
      " [ 234   13    1    3    3]\n",
      " [  44    3    2    2    1]\n",
      " [  79    4    2    1    0]\n",
      " [  36    1    0    2    1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "evaluate_model(rnn_model, X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      3205\n",
      "           1       0.12      0.04      0.06       254\n",
      "           2       0.00      0.00      0.00        52\n",
      "           3       0.00      0.00      0.00        86\n",
      "           4       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.86      3637\n",
      "   macro avg       0.20      0.20      0.20      3637\n",
      "weighted avg       0.79      0.86      0.82      3637\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3132   66    1    6    0]\n",
      " [ 244   10    0    0    0]\n",
      " [  50    2    0    0    0]\n",
      " [  83    3    0    0    0]\n",
      " [  39    0    0    1    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/huiyisang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(birnn_model, X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      3205\n",
      "           1       0.12      0.04      0.05       254\n",
      "           2       0.00      0.00      0.00        52\n",
      "           3       0.11      0.03      0.05        86\n",
      "           4       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.86      3637\n",
      "   macro avg       0.22      0.21      0.21      3637\n",
      "weighted avg       0.79      0.86      0.82      3637\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3116   62    8   16    3]\n",
      " [ 235    9    4    6    0]\n",
      " [  48    2    0    2    0]\n",
      " [  82    1    0    3    0]\n",
      " [  39    1    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lstm_model, X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/mnt/data/Recipe Reviews and User Feedback Dataset.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Inspect and preprocess the dataset\n",
    "# Normalize 'best_score' to classify into 5 classes: \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "score_bins = [0, 200, 400, 600, 800, 964]\n",
    "score_labels = [0, 1, 2, 3, 4]  # Use numerical labels to avoid encoding issues\n",
    "data['best_score_class'] = pd.cut(data['best_score'], bins=score_bins, labels=score_labels, include_lowest=True)\n",
    "\n",
    "# Combine review text and target variable\n",
    "texts = data['text'].astype(str)\n",
    "labels = data['best_score_class'].astype(int)  # Ensure labels are integers\n",
    "\n",
    "# Tokenize and pad the text\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Embedding dimensions\n",
    "embedding_dim = 128\n",
    "\n",
    "# Function to create RNN model\n",
    "def create_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        SimpleRNN(128, activation='tanh'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to create BiRNN model\n",
    "def create_birnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        Bidirectional(SimpleRNN(128, activation='tanh')),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        LSTM(128, activation='tanh'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Cross-validation setup\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate RNN model\n",
    "print(\"\\nCross-validating RNN model...\")\n",
    "rnn_keras = KerasClassifier(build_fn=create_rnn_model, epochs=5, batch_size=32, verbose=0)\n",
    "rnn_results = []\n",
    "for train_idx, test_idx in kfold.split(padded_sequences, labels):\n",
    "    rnn_keras.fit(padded_sequences[train_idx], np.array(labels)[train_idx])\n",
    "    rnn_score = rnn_keras.score(padded_sequences[test_idx], np.array(labels)[test_idx])\n",
    "    rnn_results.append(rnn_score)\n",
    "print(f\"RNN Cross-Validation Accuracy: {np.mean(rnn_results)} (+/- {np.std(rnn_results)})\")\n",
    "\n",
    "# Evaluate BiRNN model\n",
    "print(\"\\nCross-validating BiRNN model...\")\n",
    "birnn_keras = KerasClassifier(build_fn=create_birnn_model, epochs=5, batch_size=32, verbose=0)\n",
    "birnn_results = []\n",
    "for train_idx, test_idx in kfold.split(padded_sequences, labels):\n",
    "    birnn_keras.fit(padded_sequences[train_idx], np.array(labels)[train_idx])\n",
    "    birnn_score = birnn_keras.score(padded_sequences[test_idx], np.array(labels)[test_idx])\n",
    "    birnn_results.append(birnn_score)\n",
    "print(f\"BiRNN Cross-Validation Accuracy: {np.mean(birnn_results)} (+/- {np.std(birnn_results)})\")\n",
    "\n",
    "# Evaluate LSTM model\n",
    "print(\"\\nCross-validating LSTM model...\")\n",
    "lstm_keras = KerasClassifier(build_fn=create_lstm_model, epochs=5, batch_size=32, verbose=0)\n",
    "lstm_results = []\n",
    "for train_idx, test_idx in kfold.split(padded_sequences, labels):\n",
    "    lstm_keras.fit(padded_sequences[train_idx], np.array(labels)[train_idx])\n",
    "    lstm_score = lstm_keras.score(padded_sequences[test_idx], np.array(labels)[test_idx])\n",
    "    lstm_results.append(lstm_score)\n",
    "print(f\"LSTM Cross-Validation Accuracy: {np.mean(lstm_results)} (+/- {np.std(lstm_results)})\")\n",
    "\n",
    "# Save the label encoder for inference\n",
    "import pickle\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(score_labels, f)\n",
    "\n",
    "# Save the tokenizer for inference\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
